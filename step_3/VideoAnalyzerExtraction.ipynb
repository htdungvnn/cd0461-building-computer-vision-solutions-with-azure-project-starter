{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please install the required Python modules/SDKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! activate ai-azure-c1\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/opt/conda/envs/ai-azure-c1/lib/python3.8/site-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This demo uses the latest pillow package to show the rectangular bounding box around the face, so please upgrade the pillow package using the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow==8.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Useful Python Libraries or Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import io\n",
    "import glob, os, sys, time, uuid\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from video_indexer import VideoIndexer\n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from azure.cognitiveservices.vision.face.models import TrainingStatusType\n",
    "from msrest.authentication import CognitiveServicesCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "- https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-for-media-docs/video-indexer-use-apis\n",
    "\n",
    "### Set up the Video Analyzer Portal Login:\n",
    "- https://www.videoindexer.ai/account\n",
    "- https://www.videoindexer.ai/media/library  - All uploaded video\n",
    "\n",
    "### Get API Subscription:\n",
    "https://api-portal.videoindexer.ai/\n",
    "\n",
    "### Article\n",
    "- https://medium.com/microsoftazure/visual-brand-detection-with-azure-video-indexer-cd65330b908c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'SUBSCRIPTION_KEY': 'YOUR VIDEO INDEXER SUBSCRIPTION KEY',\n",
    "    'LOCATION': 'trial',\n",
    "    'ACCOUNT_ID': 'YOUR VIDEO INDEXER ACCOUNT ID'\n",
    "}\n",
    "\n",
    "video_analysis = VideoIndexer(\n",
    "    vi_subscription_key=CONFIG['SUBSCRIPTION_KEY'],\n",
    "    vi_location=CONFIG['LOCATION'],\n",
    "    vi_account_id=CONFIG['ACCOUNT_ID']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location Parameter\n",
    " - For paid service, please use service region i.e. westus2, eastus, etc.\n",
    " - For trial or free service, just use \"trial\" as I have used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_analysis.check_access_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: Please upload a video to Azure Video Indexer/Analyzer service and use its ID below instead of the given one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get video ID, check on the video in the Library and you will get the video ID, appended to the URL. An example is shown here:\n",
    "* Video URL:\n",
    "https://www.videoindexer.ai/accounts/d1629197-588b-40a7-98fa-e19785ca082e/videos/2240904ed2\n",
    "* Video ID: 2240904ed2\n",
    "* If you don't have a video at this point, you can download this video and upload it at the Video Indexer portal:\n",
    "https://github.com/udacity/cd0461-building-computer-vision-solutions-with-azure-exercises/blob/main/resources/video-indexer-demo-video.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = 'ENTER YOUR VIDEO ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_analysis.get_video_info(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = video_analysis.get_video_info(video_id, video_language='English')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing RAW Json \n",
    "### Getting a list of thumbnails where we find human face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(info['videos'][0]['insights']['faces'][0]['thumbnails']):\n",
    "    print(\"We found {} faces in this video.\".format(str(len(info['videos'][0]['insights']['faces'][0]['thumbnails']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['videos'][0]['insights']['faces'][0]['thumbnails']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Thumbnail ID from the Analysis JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "img_raw = []\n",
    "img_strs = []\n",
    "for each_thumb in info['videos'][0]['insights']['faces'][0]['thumbnails']:\n",
    "    if 'fileName' in each_thumb and 'id' in each_thumb:\n",
    "        file_name = each_thumb['fileName']\n",
    "        thumb_id = each_thumb['id']\n",
    "        img_code = video_analysis.get_thumbnail_from_video_indexer(video_id,  thumb_id)\n",
    "        img_strs.append(img_code)\n",
    "        img_stream = io.BytesIO(img_code)\n",
    "        img_raw.append(img_stream)\n",
    "        img = Image.open(img_stream)\n",
    "        images.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's view the face-specific thumbnails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in images:\n",
    "    print(img.info)\n",
    "    plt.figure()\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's extract and save these face thumbnails to the local disk \n",
    "- Download from Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for img in images:\n",
    "    print(type(img))\n",
    "    img.save('video-analyzer-face' + str(i) + '.jpg')\n",
    "    i= i+ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the download process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls video-analyzer-face*.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting thumbnail from the SDK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter one of the thumbnail output you got from the previous cell, \n",
    "# under the \"Getting Thumbnail ID from the Analysis JSON\" section.\n",
    "thumbnail_id='ENTER THUMBNAIL HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_code = video_analysis.get_thumbnail_from_video_indexer(video_id,  thumbnail_id)\n",
    "print(img_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting encoded image to visible image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_code = video_analysis.get_thumbnail_from_video_indexer(video_id,  thumbnail_id)\n",
    "img_stream = io.BytesIO(img_code)\n",
    "img = Image.open(img_stream)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Faces from  Video Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVKASH_FACE_KEY = \"YOUR FACE SERVICE KEY\"\n",
    "AVKASH_FACE_ENDPOINT = \"YOUR FACE SERVICE ENDPOINT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client\n",
    "face_client = FaceClient(AVKASH_FACE_ENDPOINT, CognitiveServicesCredentials(AVKASH_FACE_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_client.api_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Azure-Samples/cognitive-services-quickstart-code/blob/master/python/Face/DetectIdentifyFace.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Person Model Using Person Images\n",
    "**Note: here you need to build a person model using the person images from the previous exercise. Before moving forward, make sure you upload all the images to the workspace.**\n",
    "\n",
    "Modify the code below as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls human-face*.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_face_images = [file for file in glob.glob('*.jpg') if file.startswith(\"human-face\")]\n",
    "print(my_face_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in my_face_images:\n",
    "    with open(img, 'rb') as img_code:\n",
    "        img_view_ready = Image.open(img_code)\n",
    "        plt.figure()\n",
    "        plt.imshow(img_view_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note if this UUID already used earlier, you will get an error \n",
    "# Replace \"person-avkash\" with your own PersonModel name\n",
    "PERSON_GROUP_ID = str(uuid.uuid4())\n",
    "person_group_name = 'person-avkash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code is taken from Azure face SDK \n",
    "## ---------------------------------------\n",
    "def build_person_group(client, person_group_id, pgp_name):\n",
    "    print('Create and build a person group...')\n",
    "    # Create empty Person Group. Person Group ID must be lower case, alphanumeric, and/or with '-', '_'.\n",
    "    print('Person group ID:', person_group_id)\n",
    "    client.person_group.create(person_group_id = person_group_id, name=person_group_id)\n",
    "\n",
    "    # Create a person group person.\n",
    "    my_face = client.person_group_person.create(person_group_id, pgp_name)\n",
    "    # Find all jpeg images of human in working directory.\n",
    "    my_face_images = [file for file in glob.glob('*.jpg') if file.startswith(\"human-face\")]\n",
    "    # Add images to a Person object\n",
    "    for image_p in my_face_images:\n",
    "        with open(image_p, 'rb') as w:\n",
    "            client.person_group_person.add_face_from_stream(person_group_id, my_face.person_id, w)\n",
    "\n",
    "    # Train the person group, after a Person object with many images were added to it.\n",
    "    client.person_group.train(person_group_id)\n",
    "\n",
    "    # Wait for training to finish.\n",
    "    while (True):\n",
    "        training_status = client.person_group.get_training_status(person_group_id)\n",
    "        print(\"Training status: {}.\".format(training_status.status))\n",
    "        if (training_status.status is TrainingStatusType.succeeded):\n",
    "            break\n",
    "        elif (training_status.status is TrainingStatusType.failed):\n",
    "            client.person_group.delete(person_group_id=PERSON_GROUP_ID)\n",
    "            sys.exit('Training the person group has failed.')\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_person_group(face_client, PERSON_GROUP_ID, person_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect all faces in query image list, then add their face IDs to a new list.\n",
    "def detect_faces(client, query_images_list):\n",
    "    print('Detecting faces in query images list...')\n",
    "\n",
    "    face_ids = {} # Keep track of the image ID and the related image in a dictionary\n",
    "    for image_name in query_images_list:\n",
    "        image = open(image_name, 'rb') # BufferedReader\n",
    "        print(\"Opening image: \", image.name)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Detect the faces in the query images list one at a time, returns list[DetectedFace]\n",
    "        faces = client.face.detect_with_stream(image)  \n",
    "\n",
    "        # Add all detected face IDs to a list\n",
    "        for face in faces:\n",
    "            print('Face ID', face.face_id, 'found in image', os.path.splitext(image.name)[0]+'.jpg')\n",
    "            # Add the ID to a dictionary with image name as a key.\n",
    "            # This assumes there is only one face per image (since you can't have duplicate keys)\n",
    "            face_ids[image.name] = face.face_id\n",
    "\n",
    "    return face_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = detect_faces(face_client, my_face_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching face from the person model with face from Video Analyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------\n",
    "## Reading file locally\n",
    "## -------\n",
    "dl_image = open('/home/workspace/human-face4.jpg', 'rb')\n",
    "dl_faces = face_client.face.detect_with_stream(dl_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Face ID and then saving it into the list of already saved Face IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for face in dl_faces:\n",
    "    print('Face ID', face.face_id, 'found in image', dl_image)\n",
    "    # Add the ID to a dictionary with image name as a key.\n",
    "    # This assumes there is only one face per image (since you can't have duplicate keys)\n",
    "    ids['sample.png'] = face.face_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have 4 + 1 = 5 Face IDs in our Face ID list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the face ID from the identify card and matching the identity with the Person Group model\n",
    "\n",
    "### Note: Please replace the face UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the face ID of ca-dl-sample.png from the output of the cell above\n",
    "get_the_face_id_from_the_sample = 'ENTER FACE ID HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_gp_results = face_client.face.identify([get_the_face_id_from_the_sample], PERSON_GROUP_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in person_gp_results:\n",
    "    if result.candidates:\n",
    "        for candidate in result.candidates:\n",
    "            print(\"The Identity match confidence is {}\".format(candidate.confidence))\n",
    "    else:\n",
    "        print(\"Can't verify the identity with the person group\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
